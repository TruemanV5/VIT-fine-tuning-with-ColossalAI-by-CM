Epoch [1]:   0%|          | 0/32 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
Epoch [1]:  84%|████████▍ | 27/32 [00:08<00:01,  3.24it/s, loss=0.174]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
Epoch [1]: 100%|██████████| 32/32 [00:10<00:00,  3.17it/s, loss=0.137]
Evaluation result for epoch 1:                 average_loss=0.1712,                 accuracy=0.9531.
Epoch [2]: 100%|██████████| 32/32 [00:09<00:00,  3.24it/s, loss=0.0235]
Evaluation result for epoch 2:                 average_loss=0.0359,                 accuracy=0.9844.
Epoch [3]: 100%|██████████| 32/32 [00:09<00:00,  3.24it/s, loss=0.00343]
Evaluation result for epoch 3:                 average_loss=0.0319,                 accuracy=0.9922.

Epoch [1]:   0%|          | 0/32 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Epoch [1]:   3%|▎         | 1/32 [00:00<00:11,  2.73it/s, loss=1.29]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
Epoch [1]:  81%|████████▏ | 26/32 [00:02<00:00, 12.03it/s, loss=0.185]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
Epoch [1]: 100%|██████████| 32/32 [00:02<00:00, 10.82it/s, loss=0.134]
Evaluation result for epoch 1:                 average_loss=0.1436,                 accuracy=0.9531.
Epoch [2]: 100%|██████████| 32/32 [00:02<00:00, 12.15it/s, loss=0.0549]
Evaluation result for epoch 2:                 average_loss=0.0311,                 accuracy=0.9922.
Epoch [3]: 100%|██████████| 32/32 [00:02<00:00, 12.17it/s, loss=0.00328]
Evaluation result for epoch 3:                 average_loss=0.0250,                 accuracy=0.9922.

Epoch [1]:   0%|          | 0/32 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
Epoch [1]:  84%|████████▍ | 27/32 [00:04<00:00,  7.39it/s, loss=0.181]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
Epoch [1]: 100%|██████████| 32/32 [00:05<00:00,  6.40it/s, loss=0.113]
Evaluation result for epoch 1:                 average_loss=0.1527,                 accuracy=0.9453.
Epoch [2]: 100%|██████████| 32/32 [00:04<00:00,  7.44it/s, loss=0.0599]
Evaluation result for epoch 2:                 average_loss=0.0284,                 accuracy=0.9922.
Epoch [3]: 100%|██████████| 32/32 [00:04<00:00,  7.30it/s, loss=0.00302]
Evaluation result for epoch 3:                 average_loss=0.0183,                 accuracy=0.9922.

[04/12/24 12:45:24] INFO     colossalai - colossalai - INFO:                                        
                             /content/ColossalAI/examples/images/vit/vit_train_demo.py:230 main     
                    INFO     colossalai - colossalai - INFO: Start finetuning                       
Epoch [1]:  84%|████████▍ | 27/32 [00:06<00:01,  4.55it/s, loss=0.178]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
Epoch [1]: 100%|██████████| 32/32 [00:07<00:00,  4.29it/s, loss=0.121]
Evaluation result for epoch 1:                 average_loss=0.1685,                 accuracy=0.9375.
Epoch [2]: 100%|██████████| 32/32 [00:07<00:00,  4.57it/s, loss=0.0157]
Evaluation result for epoch 2:                 average_loss=0.0339,                 accuracy=0.9844.
Epoch [3]: 100%|██████████| 32/32 [00:06<00:00,  4.57it/s, loss=0.00496]
Evaluation result for epoch 3:                 average_loss=0.0254,                 accuracy=0.9844.












[04/12/24 12:52:03] INFO     colossalai - colossalai - INFO:                                        
                             /content/ColossalAI/examples/images/vit/vit_train_demo.py:230 main     
                    INFO     colossalai - colossalai - INFO: Start finetuning                       
Epoch [1]:   0%|          | 0/32 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
Epoch [1]:  84%|████████▍ | 27/32 [00:27<00:05,  1.03s/it, loss=0.143]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
Epoch [1]: 100%|██████████| 32/32 [00:33<00:00,  1.03s/it, loss=0.0392]
Evaluation result for epoch 1:                 average_loss=0.0578,                 accuracy=0.9766.
Epoch [2]: 100%|██████████| 32/32 [00:32<00:00,  1.03s/it, loss=0.0106]
Evaluation result for epoch 2:                 average_loss=0.0477,                 accuracy=0.9844.
Epoch [3]: 100%|██████████| 32/32 [00:32<00:00,  1.03s/it, loss=0.0107]
Evaluation result for epoch 3:                 average_loss=0.0095,                 accuracy=1.0000.

Epoch [1]:   0%|          | 0/32 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
Epoch [1]:  84%|████████▍ | 27/32 [00:08<00:01,  3.49it/s, loss=0.145]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
Epoch [1]: 100%|██████████| 32/32 [00:09<00:00,  3.39it/s, loss=0.0429]
Evaluation result for epoch 1:                 average_loss=0.0598,                 accuracy=0.9766.
Epoch [2]: 100%|██████████| 32/32 [00:09<00:00,  3.48it/s, loss=0.00852]
Evaluation result for epoch 2:                 average_loss=0.0347,                 accuracy=0.9844.
Epoch [3]: 100%|██████████| 32/32 [00:09<00:00,  3.48it/s, loss=0.0101]
Evaluation result for epoch 3:                 average_loss=0.0074,                 accuracy=1.0000.

Epoch [1]:   0%|          | 0/32 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
Epoch [1]:  84%|████████▍ | 27/32 [00:08<00:01,  3.48it/s, loss=0.162]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
Epoch [1]: 100%|██████████| 32/32 [00:09<00:00,  3.27it/s, loss=0.0378]
Evaluation result for epoch 1:                 average_loss=0.0578,                 accuracy=0.9766.
Epoch [2]: 100%|██████████| 32/32 [00:09<00:00,  3.47it/s, loss=0.00752]
Evaluation result for epoch 2:                 average_loss=0.0306,                 accuracy=0.9844.
Epoch [3]: 100%|██████████| 32/32 [00:09<00:00,  3.47it/s, loss=0.0161]
Evaluation result for epoch 3:                 average_loss=0.0105,                 accuracy=0.9922.

[04/12/24 13:02:57] INFO     colossalai - colossalai - INFO:                                        
                             /content/ColossalAI/examples/images/vit/vit_train_demo.py:230 main     
                    INFO     colossalai - colossalai - INFO: Start finetuning                       
Epoch [1]:  84%|████████▍ | 27/32 [00:08<00:01,  3.23it/s, loss=0.152]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
Epoch [1]: 100%|██████████| 32/32 [00:10<00:00,  3.09it/s, loss=0.0376]
Evaluation result for epoch 1:                 average_loss=0.0538,                 accuracy=0.9688.
Epoch [2]: 100%|██████████| 32/32 [00:09<00:00,  3.22it/s, loss=0.0103]
Evaluation result for epoch 2:                 average_loss=0.0417,                 accuracy=0.9844.
Epoch [3]: 100%|██████████| 32/32 [00:09<00:00,  3.22it/s, loss=0.0103]
Evaluation result for epoch 3:                 average_loss=0.0095,                 accuracy=1.0000.








